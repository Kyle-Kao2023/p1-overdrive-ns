#!/usr/bin/env python3
"""
Learner/Trainer è¨­ç½®ç¯„ä¾‹

æ¼”ç¤ºå¦‚ä½•è¨­ç½®å’Œå•Ÿå‹• v2 å­¸ç¿’è¨“ç·´ç®¡é“ã€‚
"""

import json
import os
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path

# æ¨¡æ“¬å°å…¥ (å¯¦éš›ä½¿ç”¨æ™‚å–æ¶ˆè¨»é‡‹)
# from services.decision.learner.experience_store import ExperienceStore
# from services.decision.learner.imitation import ImitationLearner
# from services.decision.learner.rl_trainer import RLTrainer

def create_sample_expert_data() -> List[Dict[str, Any]]:
    """å‰µå»ºç¯„ä¾‹å°ˆå®¶äº¤æ˜“æ•¸æ“š"""
    return [
        {
            "trade_id": "expert_001",
            "timestamp": "2025-09-14T10:15:00Z",
            "symbol": "ETHUSDT",
            "features": {
                "sigma_1m": 0.0016,
                "Z_4H": -0.8,
                "Z_1H": -0.6,
                "C_align": 0.89,
                "direction": {
                    "dir_score_htf": -0.8,
                    "dir_htf": -3,
                    "dir_score_ltf": -0.7,
                    "dir_ltf": -2
                }
            },
            "expert_decision": {
                "action": "short",
                "size_pct": 0.03,
                "confidence": 0.85,
                "reasoning": "Strong HTF bearish with good alignment"
            },
            "outcome": {
                "p_hit_actual": 0.82,
                "mae_actual": 0.0045,
                "t_hit_actual_bars": 8,
                "pnl_pct": 0.0287
            },
            "expert_rating": 5,  # 1-5 æ˜Ÿè©•åˆ†
            "market_regime": "high_vol_bear"
        },
        {
            "trade_id": "expert_002",
            "timestamp": "2025-09-14T14:30:00Z",
            "symbol": "BTCUSDT",
            "features": {
                "sigma_1m": 0.0022,
                "Z_4H": 0.9,
                "Z_1H": 0.7,
                "C_align": 0.91,
                "direction": {
                    "dir_score_htf": 0.85,
                    "dir_htf": 3,
                    "dir_score_ltf": 0.75,
                    "dir_ltf": 2
                }
            },
            "expert_decision": {
                "action": "long",
                "size_pct": 0.025,
                "confidence": 0.92,
                "reasoning": "Perfect bullish setup with strong momentum"
            },
            "outcome": {
                "p_hit_actual": 0.88,
                "mae_actual": 0.0031,
                "t_hit_actual_bars": 5,
                "pnl_pct": 0.0356
            },
            "expert_rating": 5,
            "market_regime": "trending_bull"
        },
        # æ·»åŠ æ›´å¤šç¯„ä¾‹...
    ]

def create_human_feedback_data() -> List[Dict[str, Any]]:
    """å‰µå»ºäººé¡åé¥‹åå¥½æ•¸æ“š"""
    return [
        {
            "feedback_id": "fb_001",
            "timestamp": "2025-09-14T16:00:00Z",
            "trade_pair": {
                "trade_a": {
                    "decision": "short",
                    "size_pct": 0.04,
                    "reasoning": "Technical indicators suggest downtrend"
                },
                "trade_b": {
                    "decision": "short",
                    "size_pct": 0.02,
                    "reasoning": "Conservative approach due to volatility"
                }
            },
            "human_preference": "B",  # åå¥½äº¤æ˜“ B
            "preference_reason": "Better risk management in uncertain conditions",
            "confidence": 0.8,
            "trader_experience": "expert"
        },
        {
            "feedback_id": "fb_002",
            "timestamp": "2025-09-14T17:15:00Z",
            "trade_pair": {
                "trade_a": {
                    "decision": "wait",
                    "size_pct": 0.0,
                    "reasoning": "Unclear signals, better to wait"
                },
                "trade_b": {
                    "decision": "long",
                    "size_pct": 0.015,
                    "reasoning": "Small position to capture potential upside"
                }
            },
            "human_preference": "A",
            "preference_reason": "Risk management more important than missing opportunities",
            "confidence": 0.9,
            "trader_experience": "intermediate"
        }
    ]

def setup_experience_store():
    """è¨­ç½®ç¶“é©—å­˜å„²ç³»çµ±"""
    print("ğŸ—„ï¸ è¨­ç½® Experience Store...")

    # å‰µå»ºæ•¸æ“šç›®éŒ„
    data_dir = Path("data/v2/experience")
    data_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å°ˆå®¶æ•¸æ“š
    expert_data = create_sample_expert_data()
    expert_file = data_dir / "expert_trades.jsonl"

    with open(expert_file, 'w', encoding='utf-8') as f:
        for trade in expert_data:
            f.write(json.dumps(trade, ensure_ascii=False) + '\n')

    print(f"âœ… å°ˆå®¶äº¤æ˜“æ•¸æ“šä¿å­˜åˆ°: {expert_file}")
    print(f"   åŒ…å« {len(expert_data)} ç­†å°ˆå®¶äº¤æ˜“")

    # ä¿å­˜åå¥½æ•¸æ“š
    feedback_data = create_human_feedback_data()
    feedback_file = data_dir / "human_feedback.jsonl"

    with open(feedback_file, 'w', encoding='utf-8') as f:
        for feedback in feedback_data:
            f.write(json.dumps(feedback, ensure_ascii=False) + '\n')

    print(f"âœ… äººé¡åé¥‹æ•¸æ“šä¿å­˜åˆ°: {feedback_file}")
    print(f"   åŒ…å« {len(feedback_data)} ç­†åå¥½åé¥‹")

def setup_imitation_learning():
    """è¨­ç½®æ¨¡ä»¿å­¸ç¿’"""
    print("\nğŸ“ è¨­ç½®æ¨¡ä»¿å­¸ç¿’ (Imitation Learning)...")

    config = {
        "model": {
            "architecture": "transformer",
            "hidden_dim": 256,
            "num_layers": 4,
            "num_heads": 8
        },
        "training": {
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 100,
            "validation_split": 0.2
        },
        "data": {
            "expert_trades_path": "data/v2/experience/expert_trades.jsonl",
            "min_expert_rating": 4,  # åªä½¿ç”¨ 4-5 æ˜Ÿè©•åˆ†çš„äº¤æ˜“
            "augmentation": {
                "enabled": True,
                "noise_level": 0.05
            }
        }
    }

    config_file = Path("configs/imitation_learning.yaml")

    # é€™è£¡æ‡‰è©²ä½¿ç”¨ yaml.dumpï¼Œç‚ºäº†ç°¡åŒ–ä½¿ç”¨ json
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)

    print(f"âœ… æ¨¡ä»¿å­¸ç¿’é…ç½®ä¿å­˜åˆ°: {config_file}")
    print("ğŸ“ é…ç½®è¦é»:")
    print(f"   - æ¨¡å‹æ¶æ§‹: {config['model']['architecture']}")
    print(f"   - å­¸ç¿’ç‡: {config['training']['learning_rate']}")
    print(f"   - æœ€å°å°ˆå®¶è©•åˆ†: {config['data']['min_expert_rating']}")

def setup_rlhf():
    """è¨­ç½®äººé¡åé¥‹å¼·åŒ–å­¸ç¿’"""
    print("\nğŸ¤ è¨­ç½® RLHF (Reinforcement Learning from Human Feedback)...")

    config = {
        "reward_model": {
            "architecture": "bradley_terry",
            "hidden_dim": 128,
            "comparison_pairs": 1000  # æœ€å°‘éœ€è¦çš„æ¯”è¼ƒå°æ•¸
        },
        "policy_training": {
            "algorithm": "PPO",
            "learning_rate": 0.0003,
            "clip_epsilon": 0.2,
            "value_coef": 0.5,
            "entropy_coef": 0.01
        },
        "data": {
            "feedback_path": "data/v2/experience/human_feedback.jsonl",
            "min_confidence": 0.7,  # æœ€å°ç½®ä¿¡åº¦é–¾å€¼
            "balance_preferences": True
        },
        "safety": {
            "kl_penalty": 0.1,  # KL æ•£åº¦æ‡²ç½°
            "max_reward_scale": 5.0,
            "reward_clipping": True
        }
    }

    config_file = Path("configs/rlhf.yaml")

    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)

    print(f"âœ… RLHF é…ç½®ä¿å­˜åˆ°: {config_file}")
    print("ğŸ“ é…ç½®è¦é»:")
    print(f"   - çå‹µæ¨¡å‹: {config['reward_model']['architecture']}")
    print(f"   - ç­–ç•¥ç®—æ³•: {config['policy_training']['algorithm']}")
    print(f"   - KL æ‡²ç½°: {config['safety']['kl_penalty']}")

def setup_finrl_training():
    """è¨­ç½® FinRL é›¢ç·šè¨“ç·´"""
    print("\nğŸ‹ï¸ è¨­ç½® FinRL é›¢ç·šè¨“ç·´ç’°å¢ƒ...")

    config = {
        "environment": {
            "name": "P1TradingEnv",
            "market": "crypto",
            "assets": ["BTCUSDT", "ETHUSDT", "ADAUSDT"],
            "lookback_window": 256,
            "leverage": 100
        },
        "algorithm": {
            "name": "SAC",  # Soft Actor-Critic
            "hyperparameters": {
                "learning_rate": 0.0003,
                "buffer_size": 1000000,
                "batch_size": 256,
                "tau": 0.005,
                "gamma": 0.99
            }
        },
        "training": {
            "total_timesteps": 1000000,
            "eval_episodes": 100,
            "checkpoint_frequency": 10000,
            "early_stopping": {
                "patience": 50000,
                "min_improvement": 0.01
            }
        },
        "data": {
            "historical_data_path": "data/historical/",
            "start_date": "2023-01-01",
            "end_date": "2024-12-31",
            "train_test_split": 0.8
        }
    }

    config_file = Path("configs/finrl_training.yaml")

    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)

    print(f"âœ… FinRL é…ç½®ä¿å­˜åˆ°: {config_file}")
    print("ğŸ“ é…ç½®è¦é»:")
    print(f"   - ç’°å¢ƒ: {config['environment']['name']}")
    print(f"   - ç®—æ³•: {config['algorithm']['name']}")
    print(f"   - è¨“ç·´æ­¥æ•¸: {config['training']['total_timesteps']:,}")

def create_training_scripts():
    """å‰µå»ºè¨“ç·´è…³æœ¬"""
    print("\nğŸ“œ å‰µå»ºè¨“ç·´è…³æœ¬...")

    scripts_dir = Path("scripts/v2_training")
    scripts_dir.mkdir(parents=True, exist_ok=True)

    # æ¨¡ä»¿å­¸ç¿’è…³æœ¬
    il_script = scripts_dir / "run_imitation_learning.py"
    with open(il_script, 'w') as f:
        f.write('''#!/usr/bin/env python3
"""é‹è¡Œæ¨¡ä»¿å­¸ç¿’è¨“ç·´"""

from services.decision.learner.imitation import ImitationLearner
from pathlib import Path
import yaml

def main():
    print("ğŸ“ é–‹å§‹æ¨¡ä»¿å­¸ç¿’è¨“ç·´...")

    # åŠ è¼‰é…ç½®
    with open("configs/imitation_learning.yaml") as f:
        config = yaml.safe_load(f)

    # åˆå§‹åŒ–å­¸ç¿’å™¨
    learner = ImitationLearner(config)

    # é–‹å§‹è¨“ç·´
    learner.train()

    print("âœ… æ¨¡ä»¿å­¸ç¿’è¨“ç·´å®Œæˆ")

if __name__ == "__main__":
    main()
''')

    # RLHF è…³æœ¬
    rlhf_script = scripts_dir / "run_rlhf.py"
    with open(rlhf_script, 'w') as f:
        f.write('''#!/usr/bin/env python3
"""é‹è¡Œ RLHF è¨“ç·´"""

from services.decision.learner.rl_trainer import RLTrainer
import yaml

def main():
    print("ğŸ¤ é–‹å§‹ RLHF è¨“ç·´...")

    with open("configs/rlhf.yaml") as f:
        config = yaml.safe_load(f)

    trainer = RLTrainer(config)
    trainer.train_with_human_feedback()

    print("âœ… RLHF è¨“ç·´å®Œæˆ")

if __name__ == "__main__":
    main()
''')

    print(f"âœ… è¨“ç·´è…³æœ¬å‰µå»ºå®Œæˆ:")
    print(f"   - æ¨¡ä»¿å­¸ç¿’: {il_script}")
    print(f"   - RLHF: {rlhf_script}")

def main():
    """ä¸»è¨­ç½®æµç¨‹"""
    print("ğŸš€ P1-System v2 Learner/Trainer è¨­ç½®")
    print("=" * 60)

    # æª¢æŸ¥å…ˆæ±ºæ¢ä»¶
    print("ğŸ” æª¢æŸ¥å…ˆæ±ºæ¢ä»¶...")
    required_dirs = ["data", "configs", "scripts"]
    for dir_name in required_dirs:
        Path(dir_name).mkdir(exist_ok=True)
    print("âœ… ç›®éŒ„çµæ§‹å°±ç·’")

    # è¨­ç½®å„çµ„ä»¶
    setup_experience_store()
    setup_imitation_learning()
    setup_rlhf()
    setup_finrl_training()
    create_training_scripts()

    print("\n" + "=" * 60)
    print("ğŸ‰ Learner/Trainer è¨­ç½®å®Œæˆï¼")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. æ”¶é›†è‡³å°‘ 100 ç­†å°ˆå®¶äº¤æ˜“æ•¸æ“š")
    print("2. é‹è¡Œæ¨¡ä»¿å­¸ç¿’: python scripts/v2_training/run_imitation_learning.py")
    print("3. æ”¶é›†äººé¡åå¥½åé¥‹")
    print("4. é‹è¡Œ RLHF: python scripts/v2_training/run_rlhf.py")
    print("5. è¨­ç½® FinRL æ­·å²æ•¸æ“šä¸¦é–‹å§‹é›¢ç·šè¨“ç·´")
    print("\nğŸ’¡ æç¤º: ç¢ºä¿æœ‰è¶³å¤ çš„è¨ˆç®—è³‡æº (GPU æ¨è–¦)")

if __name__ == "__main__":
    main()