# P1-System v1 åˆ° v2 å‡ç´šæŒ‡å—

> å¾ç´”é‡åŒ– Overdrive-NS å‡ç´šåˆ° AI å¢å¼·çš„ LLM å…±å­¸ç‰ˆ

## ğŸ¯ å‡ç´šæ¦‚è¦½

v2 åœ¨ä¿æŒ v1 æ ¸å¿ƒç©©å®šæ€§çš„åŸºç¤ä¸Šï¼Œå¢åŠ äº† AI å¢å¼·æ±ºç­–èƒ½åŠ›ã€‚å‡ç´šéç¨‹è¨­è¨ˆç‚º**å‘å¾Œå…¼å®¹**ï¼Œv1 API ä»å¯æ­£å¸¸ä½¿ç”¨ã€‚

### æ ¸å¿ƒæ”¹è®Š

| çµ„ä»¶ | v1 | v2 | å‘å¾Œå…¼å®¹ |
|------|----|----|----------|
| **API ç«¯é»** | `/decide/enter` | `/v2/decide/enter` | âœ… v1 ç«¯é»ä¿ç•™ |
| **æ±ºç­–å¼•æ“** | CTFG | CTFG + xLSTM + LLM | âœ… CTFG ä»ç‚ºä¸»è¦å¼•æ“ |
| **æ–¹å‘ç·¨ç¢¼** | ä¸‰å…ƒçµ„ | Likert-7 + é€£çºŒ | âœ… è‡ªå‹•è½‰æ› |
| **é…ç½®æ–‡ä»¶** | `default.yaml` | `v2.yaml` | âœ… é»˜èª v1 é…ç½® |

## ğŸ“‹ å‡ç´šå‰æª¢æŸ¥æ¸…å–®

### 1. ç³»çµ±è¦æ±‚æª¢æŸ¥

```bash
# æª¢æŸ¥ Python ç‰ˆæœ¬ (éœ€è¦ 3.11+)
python --version

# æª¢æŸ¥å¯ç”¨å…§å­˜ (v2 éœ€è¦é¡å¤– 2GB)
free -h

# æª¢æŸ¥ç£ç›¤ç©ºé–“ (æ¨¡å‹æ–‡ä»¶éœ€è¦ 5GB)
df -h

# æª¢æŸ¥ GPU (å¯é¸ï¼ŒåŠ é€Ÿ LLM æ¨ç†)
nvidia-smi  # å¦‚æœæœ‰ NVIDIA GPU
```

### 2. æ•¸æ“šå‚™ä»½

```bash
# å‚™ä»½ç•¶å‰é…ç½®
cp configs/default.yaml configs/default_v1_backup.yaml

# å‚™ä»½æ±ºç­–æ—¥èªŒ
tar -czf decision_logs_v1_$(date +%Y%m%d).tar.gz logs/

# å‚™ä»½æ•¸æ“šåº« (å¦‚æœä½¿ç”¨)
pg_dump p1_trading > p1_trading_v1_backup.sql
```

### 3. ä¾è³´æª¢æŸ¥

```bash
# æª¢æŸ¥ Docker ç‰ˆæœ¬
docker --version
docker-compose --version

# æª¢æŸ¥ç¶²çµ¡é€£æ¥ (ä¸‹è¼‰æ¨¡å‹éœ€è¦)
curl -I https://huggingface.co

# æª¢æŸ¥ Redis å¯ç”¨æ€§
redis-cli ping
```

## ğŸš€ å‡ç´šæ­¥é©Ÿ

### Step 1: åœæ­¢ v1 æœå‹™

```bash
# åœæ­¢æ‰€æœ‰ v1 æœå‹™
docker-compose down

# ç¢ºèªé€²ç¨‹å®Œå…¨åœæ­¢
ps aux | grep "uvicorn\|python" | grep -v grep
```

### Step 2: æ›´æ–°ä»£ç¢¼

```bash
# æ‹‰å–æœ€æ–° v2 ä»£ç¢¼
git fetch origin
git checkout main
git pull origin main

# æª¢æŸ¥ v2 æ–‡ä»¶å­˜åœ¨
ls -la services/decision/brains/llm_reasoner.py
ls -la services/decision/learner/
ls -la update/p1-system-v2-overdrive-ns-llm/
```

### Step 3: å®‰è£ v2 ä¾è³´

```bash
# å®‰è£æ–°ä¾è³´
pip install -r requirements-v2.txt

# å®‰è£ FinGPT æ¨¡å‹ (å¯é¸ï¼Œä½¿ç”¨æœ¬åœ° LLM)
pip install fingpt-toolkit

# ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
python scripts/download_v2_models.py
```

### Step 4: é…ç½® v2

```bash
# è¤‡è£½ v2 é…ç½®æ¨¡æ¿
cp configs/v2.yaml.example configs/v2.yaml

# ç·¨è¼¯ v2 é…ç½®
nano configs/v2.yaml
```

**é—œéµé…ç½®é …ç›®ï¼š**

```yaml
# configs/v2.yaml
version: "2.0"

# v2 åŠŸèƒ½é–‹é—œ (å¯é€æ­¥å•Ÿç”¨)
features:
  enable_llm_reasoner: true      # å•Ÿç”¨ LLM æ¨ç†å™¨
  enable_xlstm: true             # å•Ÿç”¨ xLSTM åºåˆ—å»ºæ¨¡
  enable_learner_trainer: false # ç¬¬ä¸€éšæ®µå…ˆé—œé–‰
  enable_fingpt: true            # å•Ÿç”¨æ–°èåˆ†æ
  enable_finrl: false            # é›¢ç·šè¨“ç·´æš«æ™‚é—œé–‰

# LLM æ¨ç†é…ç½®
llm_reasoner:
  provider: "fingpt"             # fingpt, openai, anthropic
  model: "fingpt-7b"
  api_key: ""                    # å¦‚æœä½¿ç”¨é›²ç«¯æœå‹™
  timeout_ms: 150                # åš´æ ¼è¶…æ™‚é™åˆ¶
  borderline_range: [0.72, 0.78] # è§¸ç™¼ LLM çš„ç¯„åœ
  max_daily_calls: 100           # æˆæœ¬æ§åˆ¶
  fallback_strategy: "conservative" # è¶…æ™‚æ™‚çš„å‚™ç”¨ç­–ç•¥

# xLSTM é…ç½®
xlstm:
  model_path: "models/xlstm_v2.pt"
  sequence_length: 256
  hidden_dim: 512
  num_layers: 6

# å­¸ç¿’å™¨é…ç½® (æš«æ™‚é—œé–‰)
learner:
  experience_buffer_size: 10000
  learning_rate: 0.001
  batch_size: 128

# v1 å…¼å®¹æ€§è¨­ç½®
compatibility:
  v1_api_enabled: true           # ä¿æŒ v1 API å¯ç”¨
  auto_convert_direction: true   # è‡ªå‹•è½‰æ›æ–¹å‘ç·¨ç¢¼
  fallback_to_v1: true           # å‡ºéŒ¯æ™‚å›é€€åˆ° v1
```

### Step 5: æ•¸æ“šé·ç§»

```bash
# é‹è¡Œ v2 æ•¸æ“šé·ç§»è…³æœ¬
python scripts/migrate_to_v2.py

# é©—è­‰æ•¸æ“šé·ç§»
python scripts/validate_v2_migration.py
```

### Step 6: å•Ÿå‹• v2 æœå‹™

```bash
# å•Ÿå‹• v2 æœå‹™ (æ··åˆæ¨¡å¼)
docker-compose -f docker-compose.v2.yml up -d

# æª¢æŸ¥æœå‹™ç‹€æ…‹
docker-compose ps
curl http://localhost:8000/health
curl http://localhost:8000/v2/health
```

## ğŸ§ª v2 åŠŸèƒ½æ¸¬è©¦

### 1. åŸºæœ¬ API æ¸¬è©¦

```bash
# æ¸¬è©¦ v1 API ä»ç„¶å·¥ä½œ
curl -X POST http://localhost:8000/decide/enter \
  -H "Content-Type: application/json" \
  -d @tests/data/v1_sample.json

# æ¸¬è©¦ v2 API
curl -X POST http://localhost:8000/v2/decide/enter \
  -H "Content-Type: application/json" \
  -d @tests/data/v2_sample.json
```

### 2. LLM æ¨ç†æ¸¬è©¦

```python
# æ¸¬è©¦é‚Šç•Œæƒ…æ³è§¸ç™¼ LLM
import requests

borderline_request = {
    "symbol": "ETHUSDT",
    "version": "2.0",
    "features": {
        "sigma_1m": 0.0018,
        "direction": {
            "dir_score_htf": -0.75,
            "dir_htf": -2,
            "dir_score_ltf": -0.65,
            "dir_ltf": -2
        }
    },
    "pgm": {
        "p_hit": 0.74,  # é‚Šç•Œå€¼ï¼Œæ‡‰è§¸ç™¼ LLM
        "mae_q999": 0.0058
    }
}

response = requests.post(
    "http://localhost:8000/v2/decide/enter",
    json=borderline_request
)

result = response.json()

# æª¢æŸ¥ LLM æ¨ç†æ˜¯å¦å•Ÿç”¨
assert "llm_reasoning" in result
print(f"LLM Rationale: {result['llm_reasoning']['rationale']}")
```

### 3. æ€§èƒ½åŸºæº–æ¸¬è©¦

```bash
# é‹è¡Œ v2 æ€§èƒ½æ¸¬è©¦
python tests/benchmark_v2.py

# æ¯”è¼ƒ v1 vs v2 å»¶é²
python tests/compare_v1_v2_latency.py
```

## ğŸ”„ é€æ­¥åŠŸèƒ½å•Ÿç”¨

### éšæ®µ 1: æ ¸å¿ƒ AI åŠŸèƒ½ (æ¨è–¦)

```yaml
# åƒ…å•Ÿç”¨æ ¸å¿ƒ AI åŠŸèƒ½
features:
  enable_llm_reasoner: true
  enable_xlstm: true
  enable_learner_trainer: false
  enable_fingpt: true
  enable_finrl: false
```

**é©—è­‰æ­¥é©Ÿï¼š**
- [ ] LLM æ¨ç†åœ¨é‚Šç•Œæƒ…æ³æ­£å¸¸å·¥ä½œ
- [ ] xLSTM åºåˆ—ç‰¹å¾µæ­£å¸¸ç”Ÿæˆ
- [ ] æ–°èæƒ…ç·’åˆ†ææ­£å¸¸é‹è¡Œ
- [ ] å»¶é²ä»åœ¨ 70ms SLA å…§

### éšæ®µ 2: å­¸ç¿’å™¨åŠŸèƒ½

```yaml
# å•Ÿç”¨å­¸ç¿’å™¨ (éœ€è¦æ¨™è¨»æ•¸æ“š)
features:
  enable_learner_trainer: true
```

**å‰ç½®éœ€æ±‚ï¼š**
- [ ] æº–å‚™ â‰¥100 ç­†äººå·¥æ¨™è¨»äº¤æ˜“
- [ ] è¨­ç½® Experience Store æ•¸æ“šåº«
- [ ] é…ç½®äººé¡åé¥‹æ”¶é›†ç•Œé¢

### éšæ®µ 3: é›¢ç·šè¨“ç·´

```yaml
# å•Ÿç”¨ FinRL é›¢ç·šè¨“ç·´
features:
  enable_finrl: true
```

**å‰ç½®éœ€æ±‚ï¼š**
- [ ] å¤§é‡æ­·å²æ•¸æ“š (â‰¥1å¹´)
- [ ] GPU è¨“ç·´ç’°å¢ƒ
- [ ] è¨“ç·´è¨ˆç®—è³‡æºé ç®—

## ğŸ› ï¸ å•é¡Œæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. LLM æ¨ç†è¶…æ™‚

**ç—‡ç‹€ï¼š** LLM æ¨ç†é »ç¹è¶…æ™‚ï¼Œå›é€€åˆ° conservative ç­–ç•¥

**è§£æ±ºæ–¹æ¡ˆï¼š**
```yaml
llm_reasoner:
  timeout_ms: 200  # å¢åŠ è¶…æ™‚æ™‚é–“
  provider: "fingpt"  # åˆ‡æ›åˆ°æœ¬åœ°æ¨¡å‹
  fallback_strategy: "use_ctfg_only"
```

#### 2. xLSTM æ¨¡å‹åŠ è¼‰å¤±æ•—

**ç—‡ç‹€ï¼š** `FileNotFoundError: models/xlstm_v2.pt`

**è§£æ±ºæ–¹æ¡ˆï¼š**
```bash
# é‡æ–°ä¸‹è¼‰æ¨¡å‹
python scripts/download_v2_models.py --force
# æˆ–ä½¿ç”¨å‚™ç”¨æ¨¡å‹
cp models/xlstm_backup.pt models/xlstm_v2.pt
```

#### 3. æ€§èƒ½é€€åŒ–

**ç—‡ç‹€ï¼š** v2 å»¶é²è¶…é 70ms SLA

**è¨ºæ–·ï¼š**
```bash
# æª¢æŸ¥å„çµ„ä»¶å»¶é²
curl http://localhost:8000/v2/metrics/latency
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
- é—œé–‰ LLM æ¨ç†: `enable_llm_reasoner: false`
- èª¿æ•´ xLSTM åƒæ•¸: `sequence_length: 128`
- ä½¿ç”¨ GPU åŠ é€Ÿ: `device: "cuda"`

#### 4. v1 API ä¸å·¥ä½œ

**ç—‡ç‹€ï¼š** v1 ç«¯é»è¿”å› 404

**è§£æ±ºæ–¹æ¡ˆï¼š**
```yaml
compatibility:
  v1_api_enabled: true  # ç¢ºä¿å•Ÿç”¨
```

## ğŸ”™ å›é€€è¨ˆåŠƒ

å¦‚æœ v2 å‡ºç¾åš´é‡å•é¡Œï¼Œå¯ä»¥å¿«é€Ÿå›é€€åˆ° v1ï¼š

### å¿«é€Ÿå›é€€

```bash
# 1. åœæ­¢ v2 æœå‹™
docker-compose -f docker-compose.v2.yml down

# 2. æ¢å¾© v1 é…ç½®
cp configs/default_v1_backup.yaml configs/default.yaml

# 3. å•Ÿå‹• v1 æœå‹™
docker-compose up -d

# 4. é©—è­‰æœå‹™æ­£å¸¸
curl http://localhost:8000/health
```

### æ•¸æ“šå›é€€

```bash
# æ¢å¾© v1 æ•¸æ“šåº«
psql p1_trading < p1_trading_v1_backup.sql

# æ¢å¾©æ—¥èªŒ
tar -xzf decision_logs_v1_*.tar.gz
```

## ğŸ“Š å‡ç´šå¾Œç›£æ§

### é—œéµæŒ‡æ¨™

```bash
# å»¶é²ç›£æ§
watch 'curl -s http://localhost:8000/v2/metrics | jq .latency_p95'

# LLM ä½¿ç”¨ç‡
watch 'curl -s http://localhost:8000/v2/metrics | jq .llm_usage'

# éŒ¯èª¤ç‡
watch 'curl -s http://localhost:8000/v2/metrics | jq .error_rate'
```

### å ±è­¦è¨­ç½®

```yaml
# alerts.yaml
alerts:
  - name: "v2_high_latency"
    condition: "latency_p95 > 70"
    action: "email,slack"

  - name: "llm_timeout_rate"
    condition: "llm_timeout_rate > 0.1"
    action: "disable_llm"

  - name: "v2_error_rate"
    condition: "error_rate > 0.05"
    action: "fallback_to_v1"
```

## ğŸ“š é€²éšé…ç½®

### å¤šæ¨¡å‹ LLM è¨­ç½®

```yaml
# æ”¯æŒå¤šå€‹ LLM æä¾›å•†
llm_reasoner:
  providers:
    - name: "fingpt"
      model: "fingpt-7b"
      weight: 0.6
    - name: "openai"
      model: "gpt-4-turbo"
      weight: 0.3
    - name: "anthropic"
      model: "claude-3-sonnet"
      weight: 0.1

  ensemble_strategy: "weighted_average"
```

### è‡ªå®šç¾©å­¸ç¿’å™¨é…ç½®

```yaml
learner:
  imitation_learning:
    data_path: "data/expert_trades.jsonl"
    model_architecture: "transformer"
    learning_rate: 0.001

  rlhf:
    preference_model: "bradley_terry"
    reward_learning_rate: 0.0005

  offline_rl:
    algorithm: "CQL"  # Conservative Q-Learning
    env_config: "finrl_crypto_100x"
```

## âœ… å‡ç´šå®Œæˆæª¢æŸ¥

- [ ] v1 API ä»ç„¶æ­£å¸¸å·¥ä½œ
- [ ] v2 API éŸ¿æ‡‰æ­£ç¢ºçš„ JSON æ ¼å¼
- [ ] LLM æ¨ç†åœ¨é‚Šç•Œæƒ…æ³æ­£å¸¸è§¸ç™¼
- [ ] xLSTM ç‰¹å¾µæå–æ­£å¸¸å·¥ä½œ
- [ ] å»¶é²ä»åœ¨ SLA ç¯„åœå…§ (<70ms)
- [ ] æ‰€æœ‰ Gate æª¢æŸ¥æ­£å¸¸é€šé
- [ ] æ–°èæƒ…ç·’åˆ†ææ­£å¸¸é‹è¡Œ
- [ ] ç›£æ§å’Œå ±è­¦é…ç½®å®Œæˆ
- [ ] å›é€€è¨ˆåŠƒæ¸¬è©¦é€šé

## ğŸ†˜ ç²å–å¹«åŠ©

å¦‚æœå‡ç´šéç¨‹ä¸­é‡åˆ°å•é¡Œï¼š

- ğŸ“§ **æŠ€è¡“æ”¯æŒ**: [v2-support@p1trading.dev](mailto:v2-support@p1trading.dev)
- ğŸ’¬ **Discord**: [#v2-upgrade é »é“](https://discord.gg/p1trading)
- ğŸ“š **æ–‡æª”**: [å®Œæ•´ v2 æ–‡æª”](README_v2.md)
- ğŸ› **å•é¡Œå ±å‘Š**: [GitHub Issues](https://github.com/user/p1-overdrive-ns/issues) (æ¨™è¨˜ `v2-upgrade`)

---

ğŸ‰ **æ­å–œï¼** æ‚¨å·²æˆåŠŸå‡ç´šåˆ° P1-System v2ï¼Œäº«å— AI å¢å¼·çš„äº¤æ˜“æ±ºç­–èƒ½åŠ›ï¼